---
title: "Data Cleaning for Preparation of Analysis in R"
author: "by Matthias Langenhan"
date: "03.10.2021"
output:
  pdf_document: 
    keep_tex: yes
  html_document:
    df_print: paged
---
# This R-markdown-file documents all data cleaning and transformation done in R for the case study
## Loading packages
For basic data wrangling, manipulation and plotting (via ggplot2) we install the tidyverse package that itself contains a lot of useful packages. You will need to install the tidyverse package manually via install.packages("tidyverse"), since doing so through a knitr document causes issues. 
Then we load it into the environment:
```{r}
library(tidyverse)
```
## Check and set current working directory
If you want run this markdown file, it is important that you change the absolute file path to wherever you saved the data files!
```{r}
setwd("C:/Users/Matthias/Desktop/Google_Data_Analytics_Capstone_Project_Container")
getwd()
```
## Importing, inspection and transformation of the data
```{r}
ContainerData <- read_csv2("rawDataContainerUnloading2019v2.csv")
glimpse(ContainerData)
```
The number of rows and columns is correct and so are the data types.
We want to narrow the data down to the columns of interest and rename the variables at the same time:
```{r}
core_data <- select(ContainerData,
                    DiffTotalPaletteCompletionTimeSeconds,
                    TeamSize,
                    ItemWeight,
                    ItemVolumeCubiccentimeter,
                    ItemQuantityPerPalette,
                    PaletteNumber)
core_data <- rename(core_data,
                    time_seconds = DiffTotalPaletteCompletionTimeSeconds,
                    team_size = TeamSize,
                    item_weight = ItemWeight,
                    item_volume = ItemVolumeCubiccentimeter,
                    palette_quantity = ItemQuantityPerPalette,
                    palette_number = PaletteNumber)
glimpse(core_data)
```

Now we check if values in columns "make sense". First, we want to know if the unique values in the columns are reasonable:
```{r}
unique(core_data$team_size)
unique(core_data$palette_quantity)
unique(core_data$palette_number)
```

Is the data range from lowest to highest value reasonable?
```{r}
summary(core_data)
```

Column time_seconds shows some questionable values (lowest value of five seconds to fully stack a palette is humanly impossible). Further investigation is needed.
Save a copy of the original data file:
```{r}
write.csv2(core_data, "core_data_v1.csv")
```
Sort data by time_seconds, ascending:
```{r}
sorted <- arrange(core_data, time_seconds)
```
Output shows that there are multiple observations in the time_seconds column that seem nonsensical:
```{r}
print(sorted$time_seconds)
```

To gage which nonsensical observations to drop, we introduce a new column that shows the time it takes to stack only one item onto a palette:
```{r}
core_data_v2 <- core_data %>%
  mutate(item_time = time_seconds / palette_quantity)
```

Show the properties of the new column:
```{r}
summary(core_data_v2)
sorted_2 <- arrange(core_data_v2, item_time)
print(sorted_2$item_time)
```

We make the ad-hoc decision to drop all observations with item_time < 3 seconds and save a copy of the original data file:
```{r}
write.csv2(core_data_v2, "core_data_v2.csv")
core_data_v3 <- filter(core_data_v2, item_time >= 3)
core_data_v3 <- rename(core_data_v3,
                       item_weight_kg = item_weight,
                       item_volume_cc = item_volume,
                       item_time_sec = item_time,
                       time_sec = time_seconds)
```

Now we have 199 observations left:
```{r}
summary(core_data_v3)
glimpse(core_data_v3)
write.csv2(core_data_v3, "core_data_v3.csv")
```
Now our data set is ready to be analyzed.